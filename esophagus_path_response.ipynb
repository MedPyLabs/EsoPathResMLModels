{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aacff38-1d25-412c-acae-b0d22c805e09",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c679bc90-de9f-4662-bcf3-62e337144555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from scipy.stats import mode\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import joblib\n",
    "import os\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176d2b06-fbec-40ac-8f3e-0b2d5483d377",
   "metadata": {},
   "source": [
    "### Varaibles important for pCR prediction\n",
    "\n",
    "- Age\n",
    "- Sex_M0_F1\n",
    "- BMI\n",
    "- cT\n",
    "- cN\n",
    "- CC_Length_cm\n",
    "- Volume_cc\n",
    "- EQD2_ab_10\n",
    "- EQD2_ab_4.9\n",
    "- Tech1\n",
    "- Gap\n",
    "- HPE_SqCC1_Adeno2 (exclude patient wit adenocarcinoma)\n",
    "- Pre_Op_CTorPET_T_status\n",
    "- Pre_Op_CTorPET_N_status\n",
    "- Pre_Op_CTorPET_O_Status\n",
    "- RTtoSurg_days\n",
    "- Chemo_New_Cis_1_PC_2_Cis5F_3_Others_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af060af4-d437-4663-9173-9515145adca5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Prepraing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f74e740-91c2-4f34-9e39-9b4cbd9eca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of specified variables\n",
    "selected_columns = {\n",
    "    'no_NA': [\n",
    "        'Age', 'Sex_M0_F1', 'BMI', 'cT', 'cN', 'CC_Length_cm', \n",
    "        'EQD2_ab_10', 'EQD2_ab_4.9', 'Tech1', 'Gap', 'HPE_SqCC1_Adeno2',\n",
    "        'RTtoSurg_days', 'Chemo_New_Cis_1_PC_2_Cis5F_3_Others_4', 'T_CR_Y0_N1', 'N_CR_Y0_N1', \n",
    "        'Overall_CR_Y0_N1'\n",
    "    ],\n",
    "    'no_PET': [\n",
    "        'Age', 'Sex_M0_F1', 'BMI', 'cT', 'cN', 'CC_Length_cm', 'Volume_cc',\n",
    "        'EQD2_ab_10', 'EQD2_ab_4.9', 'Tech1', 'Gap', 'HPE_SqCC1_Adeno2',\n",
    "        'RTtoSurg_days', 'Chemo_New_Cis_1_PC_2_Cis5F_3_Others_4', 'T_CR_Y0_N1', 'N_CR_Y0_N1', \n",
    "        'Overall_CR_Y0_N1'\n",
    "    ],\n",
    "    'only_PET_overall': [\n",
    "        'Age', 'Sex_M0_F1', 'BMI', 'cT', 'cN', 'CC_Length_cm', 'Volume_cc',\n",
    "        'EQD2_ab_10', 'EQD2_ab_4.9', 'Tech1', 'Gap', 'HPE_SqCC1_Adeno2','Pre_Op_OverallCR_Y0_N1',\n",
    "        'RTtoSurg_days', 'Chemo_New_Cis_1_PC_2_Cis5F_3_Others_4', 'T_CR_Y0_N1', 'N_CR_Y0_N1', \n",
    "        'Overall_CR_Y0_N1'\n",
    "    ],\n",
    "    'only_PET_overall_no_volume': [\n",
    "        'Age', 'Sex_M0_F1', 'BMI', 'cT', 'cN', 'CC_Length_cm','EQD2_ab_10', 'EQD2_ab_4.9', 'Tech1', \n",
    "        'Gap', 'HPE_SqCC1_Adeno2','Pre_Op_OverallCR_Y0_N1', 'RTtoSurg_days', \n",
    "        'Chemo_New_Cis_1_PC_2_Cis5F_3_Others_4', 'T_CR_Y0_N1', 'N_CR_Y0_N1', 'Overall_CR_Y0_N1'\n",
    "    ],\n",
    "    'all': [\n",
    "        'Age', 'Sex_M0_F1', 'BMI', 'cT', 'cN', 'CC_Length_cm', 'Volume_cc',\n",
    "        'EQD2_ab_10', 'EQD2_ab_4.9', 'Tech1', 'Gap', 'HPE_SqCC1_Adeno2','Pre_Op_OverallCR_Y0_N1',\n",
    "        'Pre_Op_CTPET_T_CR_Y0_N1', 'Pre_Op_CTPET_N_CR_Y0_N1',\n",
    "        'RTtoSurg_days', 'Chemo_New_Cis_1_PC_2_Cis5F_3_Others_4', 'T_CR_Y0_N1', 'N_CR_Y0_N1', \n",
    "        'Overall_CR_Y0_N1'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Columns to be used as labels\n",
    "label_columns = ['T_CR_Y0_N1', 'N_CR_Y0_N1', 'Overall_CR_Y0_N1']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b0b942-a9c1-4ff3-b82e-5ed0ab9b6bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(\"data/RGCI_5_12_22.csv\")\n",
    "df_internal_val = pd.read_csv(\"data/RGCI_Int_Validate_25_1_23.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e302f-a5a1-43a7-8673-a1d42a55796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process datasets\n",
    "def process_datasets(df, df_internal_val, selected_columns):\n",
    "    # Select the specified columns\n",
    "    df_selected = df[selected_columns].copy()\n",
    "    df_internal_val_selected = df_internal_val[selected_columns].copy()\n",
    "    \n",
    "    # Remove rows with adenocarcinoma (HPE_SqCC1_Adeno2 != 2)\n",
    "    df_selected = df_selected[df_selected['HPE_SqCC1_Adeno2'] != 2]\n",
    "    df_internal_val_selected = df_internal_val_selected[df_internal_val_selected['HPE_SqCC1_Adeno2'] != 2]\n",
    "    \n",
    "    # Remove rows with any NA values\n",
    "    df_selected = df_selected.dropna()\n",
    "    df_internal_val_selected = df_internal_val_selected.dropna()\n",
    "\n",
    "    # Check and align columns and data types\n",
    "    columns_match = df_selected.dtypes.equals(df_internal_val_selected.dtypes)\n",
    "    if not columns_match:\n",
    "        print(\"There are differences in the columns or their properties between the datasets.\")\n",
    "        # Print the differences\n",
    "        diff_df = pd.concat([df_selected.dtypes, df_internal_val_selected.dtypes], axis=1, keys=['Training', 'Validation'])\n",
    "        diff_df = diff_df[diff_df['Training'] != diff_df['Validation']]\n",
    "        print(diff_df)\n",
    "        \n",
    "        # Convert validation dataset columns to match training dataset data types\n",
    "        for column in diff_df.index:\n",
    "            if df_selected[column].dtype == 'int64' and df_internal_val_selected[column].dtype == 'float64':\n",
    "                # Ensure the column can be safely converted to integers\n",
    "                if df_internal_val_selected[column].dropna().apply(float.is_integer).all():\n",
    "                    df_internal_val_selected[column] = df_internal_val_selected[column].astype('int64')\n",
    "                else:\n",
    "                    print(f\"Column {column} contains non-integer floats and cannot be converted to int64.\")\n",
    "            else:\n",
    "                df_internal_val_selected[column] = df_internal_val_selected[column].astype(df_selected[column].dtype)\n",
    "        # Check again after conversion\n",
    "        columns_match = df_selected.dtypes.equals(df_internal_val_selected.dtypes)\n",
    "        if not columns_match:\n",
    "            print(\"There are still differences in the columns or their properties between the datasets.\")\n",
    "            diff_df = pd.concat([df_selected.dtypes, df_internal_val_selected.dtypes], axis=1, keys=['Training', 'Validation'])\n",
    "            diff_df = diff_df[diff_df['Training'] != diff_df['Validation']]\n",
    "            print(diff_df)\n",
    "        else:\n",
    "            print(\"There are no differences in the columns or their properties between the datasets.\")\n",
    "            diff_df = pd.concat([df_selected.dtypes, df_internal_val_selected.dtypes], axis=1, keys=['Training', 'Validation'])\n",
    "            diff_df = diff_df[diff_df['Training'] == diff_df['Validation']]\n",
    "            print(diff_df)\n",
    "    return df_selected, df_internal_val_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dc3888-7d52-47f6-b96f-24e93208236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each dataset\n",
    "df_selected_no_NA, df_internal_val_selected_no_NA = process_datasets(df, df_internal_val, selected_columns['no_NA'])\n",
    "df_selected_no_PET, df_internal_val_selected_no_PET = process_datasets(df, df_internal_val, selected_columns['no_PET'])\n",
    "df_selected_only_PET_overall_no_volume, df_internal_val_selected_only_PET_overall_no_volume = process_datasets(df, df_internal_val, selected_columns['only_PET_overall_no_volume'])\n",
    "df_selected_only_PET_overall, df_internal_val_selected_only_PET_overall = process_datasets(df, df_internal_val, selected_columns['only_PET_overall'])\n",
    "df_selected_all, df_internal_val_selected_all = process_datasets(df, df_internal_val, selected_columns['all'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a866ab53-255b-4626-ab95-38b8d7592548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and labels (y) for the validation set\n",
    "X_val = df_internal_val_selected_no_NA.drop(columns=label_columns)\n",
    "y_val = df_internal_val_selected_no_NA[label_columns]\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(f\"Validation set: X_val shape = {X_val.shape}, y_val shape = {y_val.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ada88f-f551-453e-bdcd-6fa3d99ceae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and labels (y) for each training set\n",
    "training_sets = {\n",
    "    'df_selected_no_NA': df_selected_no_NA,\n",
    "    'df_selected_no_PET': df_selected_no_PET,\n",
    "    'df_selected_only_PET_overall_no_volume': df_selected_only_PET_overall_no_volume,\n",
    "    'df_selected_only_PET_overall': df_selected_only_PET_overall,\n",
    "    'df_selected_all': df_selected_all\n",
    "}\n",
    "\n",
    "for name, df_train in training_sets.items():\n",
    "    X_train = df_train.drop(columns=label_columns)\n",
    "    y_train = df_train[label_columns]\n",
    "    print(f\"{name}: X_train shape = {X_train.shape}, y_train shape = {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9436084f-b627-480c-9ea6-80a00cc60bd3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11073ec4-dd5c-4977-aaa6-1c3d24baee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature categories\n",
    "categorical_features = ['Sex_M0_F1','cT', 'cN','Tech1','HPE_SqCC1_Adeno2','Pre_Op_OverallCR_Y0_N1',\n",
    "        'Pre_Op_CTPET_T_CR_Y0_N1', 'Pre_Op_CTPET_N_CR_Y0_N1','Chemo_New_Cis_1_PC_2_Cis5F_3_Others_4']\n",
    "continuous_features = ['Age','BMI','CC_Length_cm','Volume_cc','EQD2_ab_10','EQD2_ab_4.9','Gap','RTtoSurg_days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb4f37-bc01-4045-ae31-c53149dcfc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipelines for numerical and categorical features\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_pipeline, continuous_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacf6518-3110-43e0-aaa3-2d9e9c09b40c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0e43715-ab51-44db-a35f-c9d09c12e42b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Creating features and labels for complete case analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e86e74-82f5-40f6-900b-a1a7d2576e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process datasets and separate features and labels\n",
    "def process_and_separate(df, selected_columns, label_columns):\n",
    "    # Select the specified columns\n",
    "    df_selected = df[selected_columns].copy()\n",
    "    \n",
    "    # Remove rows with adenocarcinoma (HPE_SqCC1_Adeno2 != 2)\n",
    "    df_selected = df_selected[df_selected['HPE_SqCC1_Adeno2'] != 2]\n",
    "    \n",
    "    # Remove rows with any NA values\n",
    "    df_selected = df_selected.dropna()\n",
    "    \n",
    "    # Separate features (X) and labels (y)\n",
    "    X = df_selected.drop(columns=label_columns)\n",
    "    y1 = df_selected['T_CR_Y0_N1']\n",
    "    y2 = df_selected['N_CR_Y0_N1']\n",
    "    y3 = df_selected['Overall_CR_Y0_N1']\n",
    "    \n",
    "    return X, y1, y2, y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256089a2-4e8b-4915-b274-4be896293fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(\"data/RGCI_5_12_22.csv\")\n",
    "df_internal_val = pd.read_csv(\"data/RGCI_Int_Validate_25_1_23.csv\")\n",
    "\n",
    "selected_columns = [\n",
    "    'Age', 'Sex_M0_F1', 'BMI', 'cT', 'cN', 'CC_Length_cm', 'Volume_cc',\n",
    "    'EQD2_ab_10', 'EQD2_ab_4.9', 'Tech1', 'Gap', 'HPE_SqCC1_Adeno2',\n",
    "    'Pre_Op_OverallCR_Y0_N1', 'Pre_Op_CTPET_T_CR_Y0_N1',\n",
    "    'Pre_Op_CTPET_N_CR_Y0_N1', 'RTtoSurg_days',\n",
    "    'Chemo_New_Cis_1_PC_2_Cis5F_3_Others_4', 'T_CR_Y0_N1', 'N_CR_Y0_N1',\n",
    "    'Overall_CR_Y0_N1'\n",
    "]\n",
    "# Columns to be used as labels\n",
    "label_columns = ['T_CR_Y0_N1', 'N_CR_Y0_N1', 'Overall_CR_Y0_N1']\n",
    "\n",
    "\n",
    "\n",
    "# Process the dataset and separate features and labels\n",
    "X, y1, y2, y3 = process_and_separate(df, selected_columns, label_columns)\n",
    "X_int_val, y1_int_val, y2_int_val, y3_int_val = process_and_separate(df_internal_val,selected_columns,label_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daef2d0-7e93-43d9-b382-7889f6a07af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d449fc0-9aab-4e65-897b-9c56a4cecbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the shapes of the resulting datasets\n",
    "print(f\"Features (X) shape = {X.shape}\")\n",
    "print(f\"Label y1 shape = {y1.shape}\")\n",
    "print(f\"Label y2 shape = {y2.shape}\")\n",
    "print(f\"Label y3 shape = {y3.shape}\")\n",
    "\n",
    "print(f\"Features (X_int_val) shape = {X_int_val.shape}\")\n",
    "print(f\"Label y1_int_val shape = {y1_int_val.shape}\")\n",
    "print(f\"Label y2_int_val shape = {y2_int_val.shape}\")\n",
    "print(f\"Label y3_int_val shape = {y3_int_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1257f65-37a3-4db6-931d-592433b5d775",
   "metadata": {},
   "source": [
    "# Model Building and HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "950e951c-fc44-4912-9b09-bad8178748fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "roc_auc_score, roc_curve, auc, confusion_matrix, classification_report)\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"data/RGCI_5_12_22.csv\")\n",
    "df_internal_val = pd.read_csv(\"data/RGCI_Int_Validate_25_1_23.csv\")\n",
    "\n",
    "# Define the features and labels, excluding 'HPE_SqCC1_Adeno2'\n",
    "selected_columns = [\n",
    "    'Age', 'Sex_M0_F1', 'BMI', 'cT', 'cN', 'CC_Length_cm', 'Volume_cc',\n",
    "    'EQD2_ab_10', 'EQD2_ab_4.9', 'Tech1', 'Gap',\n",
    "    'Pre_Op_OverallCR_Y0_N1', 'Pre_Op_CTPET_T_CR_Y0_N1',\n",
    "    'Pre_Op_CTPET_N_CR_Y0_N1', 'RTtoSurg_days',\n",
    "    'Chemo_New_Cis_1_PC_2_Cis5F_3_Others_4'\n",
    "]\n",
    "\n",
    "label_columns = ['T_CR_Y0_N1', 'N_CR_Y0_N1', 'Overall_CR_Y0_N1']\n",
    "\n",
    "# Define the categorical features\n",
    "categorical_features = [\n",
    "    'Sex_M0_F1', 'cT', 'cN', 'Tech1', 'Pre_Op_OverallCR_Y0_N1', 'Pre_Op_CTPET_T_CR_Y0_N1',\n",
    "    'Pre_Op_CTPET_N_CR_Y0_N1', 'Chemo_New_Cis_1_PC_2_Cis5F_3_Others_4'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd09a49f-7256-4576-bc02-f87021154788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the numerical features\n",
    "numerical_features = list(set(selected_columns) - set(categorical_features))\n",
    "# Filter patients with HPE_SqCC1_Adeno2 == 1 and drop this column\n",
    "df_filtered = df[df['HPE_SqCC1_Adeno2'] == 1].drop(columns=['HPE_SqCC1_Adeno2'])\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "X = df_filtered[selected_columns]\n",
    "y1 = df_filtered['T_CR_Y0_N1']\n",
    "y2 = df_filtered['N_CR_Y0_N1']\n",
    "y3 = df_filtered['Overall_CR_Y0_N1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aebdd69-d86f-4edb-94fa-7cb816c768b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the numerical pipeline\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define the categorical pipeline\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine numerical and categorical pipelines\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "# model_pipeline = Pipeline([\n",
    "#         ('preprocessor', preprocessor),\n",
    "        # ('variance_threshold', VarianceThreshold()),  # Remove constant features\n",
    "        # ('feature_selection', SelectKBest(score_func=f_classif, k=10)),\n",
    "    #     ('classifier', RandomForestClassifier())\n",
    "    # ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a51b38-c738-47cb-b9da-fb8dffcb2945",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization for RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "623ab4e5-7622-48ae-8926-9a6b87548541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y3, test_size=0.2, random_state=42)  # Using y3 as an label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca75ca30-7de7-4f26-897a-14373bc0d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 10, 50),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "    }\n",
    "    # Define the feature selection and model pipeline\n",
    "    \n",
    "    rf_clf = make_pipeline(preprocessor, RandomForestClassifier(**params, random_state=42))\n",
    "    cv_roc_auc = cross_val_score(rf_clf, X_train, y_train, cv=10, scoring='roc_auc').mean()\n",
    "    return cv_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553ea5d6-a981-4c5f-8f54-4659cad0327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of random seeds\n",
    "random_seeds  = [42, 50, 58, 66, 72]\n",
    "# Loop over the random seeds\n",
    "for i, seed in enumerate(random_seeds):\n",
    "    study_name = f\"RF_one_day_assessment_final_{i}\"\n",
    "    storage_name = \"sqlite:///db.sqlite3_rf\"\n",
    "    sampler = TPESampler(seed=seed)\n",
    "\n",
    "    # Check if study exists, if not create a new one\n",
    "    try:\n",
    "        study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "    except KeyError:\n",
    "        study = optuna.create_study(direction='maximize', sampler=sampler, storage=storage_name, study_name=study_name)\n",
    "\n",
    "    # Optimize the study\n",
    "    study.optimize(objective, n_trials=1000)\n",
    "\n",
    "    # Print the best parameters and value for each run\n",
    "    print(f\"Study {i}: Best value: {study.best_value} (params: {study.best_params})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd17e3fb-f677-414b-b74b-15ffafff0e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_study = optuna.create_study(study_name=\"RF_one_day_assessment_final04\", storage=storage_name, load_if_exists=True)\n",
    "# best_params = loaded_study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb07a8aa-9663-4213-b3b5-410460990a21",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93241e4f-4ae8-4706-9458-430683df8251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# HYPERPARAMETER OPTIMIZATION\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
    "        'lambda': trial.suggest_float('lambda', 0.0, 5.0),\n",
    "        'alpha': trial.suggest_float('alpha', 0.0, 5.0)\n",
    "    }\n",
    "    xgb_clf = make_pipeline(preprocessor, xgb.XGBClassifier(**params, use_label_encoder=False, eval_metric='logloss'))\n",
    "    cv_roc_auc = cross_val_score(xgb_clf, X_train, y_train, cv=10, scoring='roc_auc').mean()\n",
    "    return cv_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea0808d-9e89-446e-a108-4ba2b1ecbee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of random seeds\n",
    "random_seeds  = [42, 50, 58, 66, 72]\n",
    "\n",
    "# Loop over the random seeds\n",
    "for i, seed in enumerate(random_seeds):\n",
    "    study_name = f\"XGB_one_day_assessment_final_{i}\"\n",
    "    storage_name = \"sqlite:///db.sqlite3_xgb\"\n",
    "    sampler = TPESampler(seed=seed)\n",
    "\n",
    "    # Check if study exists, if not create a new one\n",
    "    try:\n",
    "        study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "    except KeyError:\n",
    "        study = optuna.create_study(direction='maximize', sampler=sampler, storage=storage_name, study_name=study_name)\n",
    "\n",
    "    # Optimize the study\n",
    "    study.optimize(objective_xgb, n_trials=1000)\n",
    "\n",
    "    # Print the best parameters and value for each run\n",
    "    print(f\"Study {i}: Best value: {study.best_value} (params: {study.best_params})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a80f72a-af4b-4c03-b126-3cd2e5246817",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_name = \"sqlite:///db.sqlite3_xgb\"\n",
    "loaded_study = optuna.create_study(study_name=\"XGB_one_day_assessment_final01\", storage=storage_name, load_if_exists=True)\n",
    "best_params = loaded_study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cce231a-2b65-4f28-89ed-c98cee50ad39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c79decdb-fd61-436c-8345-a39be5dadffa",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization for CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f4f00c2-1648-4723-a91a-3b5628864214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "def objective_cb(trial):\n",
    "    params = {\n",
    "    'iterations': trial.suggest_int('iterations', 50, 1000),\n",
    "    'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3),\n",
    "    'depth': trial.suggest_int('depth', 1, 16),\n",
    "    'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10),\n",
    "    'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 10.0),\n",
    "    'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "    'random_strength': trial.suggest_float('random_strength', 0.0, 10.0),\n",
    "    'rsm': trial.suggest_float('rsm', 0.5, 1.0),\n",
    "    'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations', 1, 10),\n",
    "    'boosting_type': trial.suggest_categorical('boosting_type', ['Ordered', 'Plain']),\n",
    "    'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "    'od_wait': trial.suggest_int('od_wait', 10, 50),\n",
    "    'verbose': 0  # Suppress output\n",
    "}\n",
    "    catboost_clf = make_pipeline(preprocessor, CatBoostClassifier(**params))\n",
    "    cv_roc_auc = cross_val_score(catboost_clf, X_train, y_train, cv=10, scoring='roc_auc').mean()\n",
    "    return cv_roc_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7877ae9-f031-40e1-a221-5cc9e515aa27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-09 06:44:46,711] A new study created in RDB with name: CatBoost_one_day_assessment_final_0\n"
     ]
    }
   ],
   "source": [
    "# List of random seeds\n",
    "random_seeds  = [42, 50, 58, 66, 72]\n",
    "\n",
    "# Loop over the random seeds\n",
    "for i, seed in enumerate(random_seeds):\n",
    "    study_name = f\"CatBoost_one_day_assessment_final_{i}\"\n",
    "    storage_name = \"sqlite:///db.sqlite3_cb\"\n",
    "    sampler = TPESampler(seed=seed)\n",
    "\n",
    "    # Check if study exists, if not create a new one\n",
    "    try:\n",
    "        study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "    except KeyError:\n",
    "        study = optuna.create_study(direction='maximize', sampler=sampler, storage=storage_name, study_name=study_name)\n",
    "\n",
    "    # Optimize the study\n",
    "    study.optimize(objective_cb, n_trials=1000)\n",
    "\n",
    "    # Print the best parameters and value for each run\n",
    "    print(f\"Study {i}: Best value: {study.best_value} (params: {study.best_params})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2840498a-b571-4c90-adb7-24e5618220cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50eb1366-4933-4238-b1ed-d4064303b6b2",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8600525-5f8b-408b-a1b3-4c5c86c64507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lgbm(trial):\n",
    "    params = {\n",
    "    'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
    "    'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3),\n",
    "    'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "    'max_depth': trial.suggest_int('max_depth', -1, 50),  # -1 means no limit\n",
    "    'min_child_samples': trial.suggest_int('min_child_samples', 1, 100),\n",
    "    'min_child_weight': trial.suggest_float('min_child_weight', 1e-3, 10),\n",
    "    'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "    'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "    'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n",
    "    'max_bin': trial.suggest_int('max_bin', 10, 500),\n",
    "    'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart', 'goss']),\n",
    "    'verbose': -1  # Suppress output\n",
    "}\n",
    "\n",
    "    lgbm_clf = make_pipeline(preprocessor, lgb.LGBMClassifier(**params))\n",
    "    cv_roc_auc = cross_val_score(lgbm_clf, X_t, y_t, cv=10, scoring='roc_auc').mean()\n",
    "    return cv_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a560d957-13e7-4c62-ad9d-72314c998d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of random seeds\n",
    "random_seeds  = [42, 50, 58, 66, 72]\n",
    "\n",
    "# Loop over the random seeds\n",
    "for i, seed in enumerate(random_seeds):\n",
    "    study_name = f\"LGBM_one_day_assessment_final_{i}\"\n",
    "    storage_name = \"sqlite:///db.sqlite3_lgbm\"\n",
    "    sampler = TPESampler(seed=seed)\n",
    "\n",
    "    # Check if study exists, if not create a new one\n",
    "    try:\n",
    "        study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "    except KeyError:\n",
    "        study = optuna.create_study(direction='maximize', sampler=sampler, storage=storage_name, study_name=study_name)\n",
    "\n",
    "    # Optimize the study\n",
    "    study.optimize(objective_lgbm, n_trials=1000)\n",
    "\n",
    "    # Print the best parameters and value for each run\n",
    "    print(f\"Study {i}: Best value: {study.best_value} (params: {study.best_params})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa770933-e500-4710-afdb-65e2a7e537de",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f825912-9c84-4376-b8ce-dafb9845c82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Nested cross-validation for hyperparameter optimization\n",
    "def objective_svm(trial):\n",
    "    params = {\n",
    "        'C': trial.suggest_float('C', 0.1, 10.0),\n",
    "        'kernel': trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "        'degree': trial.suggest_int('degree', 2, 5) if trial.params['kernel'] == 'poly' else 3,\n",
    "        'gamma': trial.suggest_categorical('gamma', ['scale', 'auto']),\n",
    "        'coef0': trial.suggest_float('coef0', 0.0, 1.0) if trial.params['kernel'] == 'poly' or trial.params['kernel'] == 'sigmoid' else 0.0\n",
    "    }\n",
    "    svc_clf = make_pipeline(preprocessor, SVC(**params, probability=True))\n",
    "    cv_roc_auc = cross_val_score(svc_clf, X_t, y_t, cv=10, scoring='roc_auc').mean()\n",
    "    return cv_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d88974d-6945-479d-9175-4687322ff8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of random seeds\n",
    "random_seeds  = [42, 50, 58, 66, 72]\n",
    "\n",
    "# Loop over the random seeds\n",
    "for i, seed in enumerate(random_seeds):\n",
    "    study_name = f\"SVM_one_day_assessment_final_{i}\"\n",
    "    storage_name = \"sqlite:///db.sqlite3_svm\"\n",
    "    sampler = TPESampler(seed=seed)\n",
    "\n",
    "    # Check if study exists, if not create a new one\n",
    "    try:\n",
    "        study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "    except KeyError:\n",
    "        study = optuna.create_study(direction='maximize', sampler=sampler, storage=storage_name, study_name=study_name)\n",
    "\n",
    "    # Optimize the study\n",
    "    study.optimize(objective_svm, n_trials=1000)\n",
    "\n",
    "    # Print the best parameters and value for each run\n",
    "    print(f\"Study {i}: Best value: {study.best_value} (params: {study.best_params})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed2be82-1225-46fb-8be0-3efc718a17ce",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6118f576-2d12-4e55-a9b7-0afc9404a59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lr(trial):\n",
    "    params = {\n",
    "        'C': trial.suggest_float('C', 0.01, 10.0),\n",
    "        'solver': trial.suggest_categorical('solver', ['lbfgs', 'liblinear', 'sag', 'saga']),\n",
    "        'max_iter': trial.suggest_int('max_iter', 100, 1000)\n",
    "    }\n",
    "    lr_clf = make_pipeline(preprocessor, LogisticRegression(**params))\n",
    "    cv_roc_auc = cross_val_score(lr_clf, X_t, y_t, cv=10, scoring='roc_auc').mean()\n",
    "    return cv_roc_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbd9aff-0400-4e4a-9046-5d1a1437875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of random seeds\n",
    "random_seeds  = [42, 50, 58, 66, 72]\n",
    "\n",
    "# Loop over the random seeds\n",
    "for i, seed in enumerate(random_seeds):\n",
    "    study_name = f\"LR_one_day_assessment_final_{i}\"\n",
    "    storage_name = \"sqlite:///db.sqlite3_lr\"\n",
    "    sampler = TPESampler(seed=seed)\n",
    "\n",
    "    # Check if study exists, if not create a new one\n",
    "    try:\n",
    "        study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "    except KeyError:\n",
    "        study = optuna.create_study(direction='maximize', sampler=sampler, storage=storage_name, study_name=study_name)\n",
    "\n",
    "    # Optimize the study\n",
    "    study.optimize(objective_lr, n_trials=1000)\n",
    "\n",
    "    # Print the best parameters and value for each run\n",
    "    print(f\"Study {i}: Best value: {study.best_value} (params: {study.best_params})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63018f71-21a3-425a-bf74-d5c75b641d07",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50d6190-8f69-494e-8c10-2c221921d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested cross-validation for hyperparameter optimization\n",
    "def objective_knn(trial):\n",
    "    params = {\n",
    "        'n_neighbors': trial.suggest_int('n_neighbors', 1, 50),\n",
    "        'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),\n",
    "        'algorithm': trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute']),\n",
    "        'leaf_size': trial.suggest_int('leaf_size', 10, 100),\n",
    "        'p': trial.suggest_int('p', 1, 2)\n",
    "    }\n",
    "    knn_clf = make_pipeline(preprocessor, KNeighborsClassifier(**params))\n",
    "    cv_roc_auc = cross_val_score(knn_clf, X_t, y_t, cv=10, scoring='roc_auc').mean()\n",
    "    return cv_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1e6fe3-856a-4e79-9794-1ddb9dc05138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of random seeds\n",
    "random_seeds  = [42, 50, 58, 66, 72]\n",
    "\n",
    "# Loop over the random seeds\n",
    "for i, seed in enumerate(random_seeds):\n",
    "    study_name = f\"KNN_one_day_assessment_final_{i}\"\n",
    "    storage_name = \"sqlite:///db.sqlite3_knn\"\n",
    "    sampler = TPESampler(seed=seed)\n",
    "\n",
    "    # Check if study exists, if not create a new one\n",
    "    try:\n",
    "        study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "    except KeyError:\n",
    "        study = optuna.create_study(direction='maximize', sampler=sampler, storage=storage_name, study_name=study_name)\n",
    "\n",
    "    # Optimize the study\n",
    "    study.optimize(objective_knn, n_trials=1000)\n",
    "\n",
    "    # Print the best parameters and value for each run\n",
    "    print(f\"Study {i}: Best value: {study.best_value} (params: {study.best_params})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325f94b2-b046-4a04-8192-af38bc2b2dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502bf02d-cb7c-42fa-b9e1-62af20417992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e53acd3-e872-4e64-8cab-2e31ff5c6d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd79e0ff-b96e-4cbc-b416-8d03e8614a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75d2c92f-4e2b-4579-94e5-25bb0fcd3d96",
   "metadata": {},
   "source": [
    "# Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949b195a-406d-4f72-b793-a445c0d43e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319c79ce-9334-4cef-8784-1cb99e48d8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and labels\n",
    "# selected_columns = [\n",
    "#     'Age', 'Sex_M0_F1', 'BMI', 'cT', 'cN', 'CC_Length_cm', 'Volume_cc',\n",
    "#     'EQD2_ab_10', 'EQD2_ab_4.9', 'Tech1', 'Gap',\n",
    "#     'Pre_Op_OverallCR_Y0_N1', 'Pre_Op_CTPET_T_CR_Y0_N1',\n",
    "#     'Pre_Op_CTPET_N_CR_Y0_N1', 'RTtoSurg_days',\n",
    "#     'Chemo_New_Cis_1_PC_2_Cis5F_3_Others_4'\n",
    "# ]\n",
    "\n",
    "selected_columns = ['Age', 'Sex_M0_F1', 'BMI', 'cT', 'cN', 'CC_Length_cm', 'EQD2_ab_10',\n",
    "       'EQD2_ab_4.9', 'Tech1', 'Gap', 'Pre_Op_OverallCR_Y0_N1', 'RTtoSurg_days',\n",
    "       'Chemo_New_Cis_1_PC_2_Cis5F_3_Others_4']\n",
    "\n",
    "label_columns = ['T_CR_Y0_N1', 'N_CR_Y0_N1', 'Overall_CR_Y0_N1']\n",
    "\n",
    "# Define the categorical features\n",
    "# categorical_features = [\n",
    "#     'Sex_M0_F1', 'cT', 'cN', 'Tech1', 'Pre_Op_OverallCR_Y0_N1', 'Pre_Op_CTPET_T_CR_Y0_N1',\n",
    "#     'Pre_Op_CTPET_N_CR_Y0_N1', 'Chemo_New_Cis_1_PC_2_Cis5F_3_Others_4'\n",
    "# ]\n",
    "\n",
    "categorical_features = [\n",
    "    'Sex_M0_F1', 'cT', 'cN', 'Tech1', 'Pre_Op_OverallCR_Y0_N1', 'Chemo_New_Cis_1_PC_2_Cis5F_3_Others_4'\n",
    "]\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"data/RGCI_5_12_22.csv\")\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "df_internal_val = pd.read_csv(\"data/RGCI_Int_Validate_25_1_23.csv\")\n",
    "\n",
    "# Filter patients with HPE_SqCC1_Adeno2 == 1 and drop this column\n",
    "# df_filtered = df[df['HPE_SqCC1_Adeno2'] == 1].drop(columns=['HPE_SqCC1_Adeno2'])\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "X = df_selected_only_PET_overall_no_volume[selected_columns]\n",
    "y3 = df_selected_only_PET_overall_no_volume['Overall_CR_Y0_N1']\n",
    "\n",
    "# Define the numerical features\n",
    "numerical_features = list(set(selected_columns) - set(categorical_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99394bae-c440-465b-8c90-a2a9489ed0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_selected_only_PET_overall_no_volume.drop(columns=['HPE_SqCC1_Adeno2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcff9bd9-add1-4136-96be-777e0933c510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the numerical pipeline\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define the categorical pipeline\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine numerical and categorical pipelines\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "# Apply preprocessing\n",
    "X_preprocessed = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f9c82-5695-4577-8de9-3a5faa7ea418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d641228e-3922-484d-82e6-1f5acd436212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67324be-1631-41d2-96e5-c5e4e67b2a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_preprocessed)\n",
    "\n",
    "# Add cluster labels to the original dataframe\n",
    "df_selected_only_PET_overall_no_volume['Cluster'] = clusters\n",
    "\n",
    "# Analyze clusters with respect to y3\n",
    "cluster_analysis = df_selected_only_PET_overall_no_volume.groupby('Cluster')[label_columns + ['Overall_CR_Y0_N1']].mean()\n",
    "print(cluster_analysis)\n",
    "\n",
    "# Chi-square test to check for significant differences in y3 across clusters\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "contingency_table = pd.crosstab(df_selected_only_PET_overall_no_volume['Cluster'], df_selected_only_PET_overall_no_volume['Overall_CR_Y0_N1'])\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(f\"Chi-square test p-value: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515db778-4e64-4798-8c75-e11509655d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality for visualization using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_preprocessed)\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=clusters, palette='viridis')\n",
    "plt.title('Clusters visualization')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Visualize the relationship between clusters and y3\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Cluster', hue='Overall_CR_Y0_N1', data=df_selected_only_PET_overall_no_volume)\n",
    "plt.title('Cluster vs Pathological Complete Response (y3)')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Pathological Complete Response (y3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2284f42-ed5e-4d0c-af35-b007aaa6e6ac",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0f5b4e-e0c0-436c-819e-826d40337b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "hierarchical = AgglomerativeClustering(n_clusters=5)\n",
    "clusters_hierarchical = hierarchical.fit_predict(X_preprocessed)\n",
    "\n",
    "# Add cluster labels to the original dataframe\n",
    "df_selected_only_PET_overall_no_volume['Cluster_Hierarchical'] = clusters_hierarchical\n",
    "\n",
    "# Analyze clusters with respect to y3\n",
    "cluster_analysis_hierarchical = df_selected_only_PET_overall_no_volume.groupby('Cluster_Hierarchical')[label_columns + ['Overall_CR_Y0_N1']].mean()\n",
    "print(cluster_analysis_hierarchical)\n",
    "\n",
    "# Chi-square test to check for significant differences in y3 across clusters\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "contingency_table = pd.crosstab(df_selected_only_PET_overall_no_volume['Cluster_Hierarchical'], df_selected_only_PET_overall_no_volume['Overall_CR_Y0_N1'])\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(f\"Chi-square test p-value: {p}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe56167-170e-40fc-8767-d2cdca78e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality for visualization using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_preprocessed)\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=clusters_hierarchical, palette='viridis')\n",
    "plt.title('Clusters visualization')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend(title='Cluster Hierarchical')\n",
    "plt.show()\n",
    "\n",
    "# Visualize the relationship between clusters and y3\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Cluster_Hierarchical', hue='Overall_CR_Y0_N1', data=df_selected_only_PET_overall_no_volume)\n",
    "plt.title('Cluster vs Pathological Complete Response (y3)')\n",
    "plt.xlabel('Cluster_Hierarchical')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Pathological Complete Response (y3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40463c8b-71f2-49e7-bc75-f3e2683ee0d4",
   "metadata": {},
   "source": [
    "# Gaussian Mixed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eecc4d-cc6b-441e-bad1-3a7744a4e3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Perform GMM clustering\n",
    "gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "clusters_gmm = gmm.fit_predict(X_preprocessed)\n",
    "\n",
    "# Add cluster labels to the original dataframe\n",
    "df_selected_only_PET_overall_no_volume['Cluster_GMM'] = clusters_gmm\n",
    "\n",
    "# Analyze clusters with respect to y3\n",
    "cluster_analysis_gmm = df_selected_only_PET_overall_no_volume.groupby('Cluster_GMM')[label_columns + ['Overall_CR_Y0_N1']].mean()\n",
    "print(cluster_analysis_gmm)\n",
    "\n",
    "# Chi-square test to check for significant differences in y3 across clusters\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "contingency_table = pd.crosstab(df_selected_only_PET_overall_no_volume['Cluster_GMM'], df_selected_only_PET_overall_no_volume['Overall_CR_Y0_N1'])\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(f\"Chi-square test p-value: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f89222-aae6-4459-ada4-0b1ac24adbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality for visualization using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_preprocessed)\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=clusters_gmm, palette='viridis')\n",
    "plt.title('Clusters visualization')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend(title='Cluster GMM')\n",
    "plt.show()\n",
    "\n",
    "# Visualize the relationship between clusters and y3\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Cluster_GMM', hue='Overall_CR_Y0_N1', data=df_selected_only_PET_overall_no_volume)\n",
    "plt.title('Cluster vs Pathological Complete Response (y3)')\n",
    "plt.xlabel('Cluster GMM')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Pathological Complete Response (y3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5471551-e21c-463f-9467-98419eb67c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbadfff4-d1cb-430f-837c-6f63e1dbc4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184e1b36-9c44-4100-8997-b58cc031b0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a25ecb-ab67-4839-9137-a7ab55cff73b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970bb738-af06-4035-a5cb-d7d6b438af56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c824670-e508-4fda-b74c-64330000d375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1306df89-a2f7-403a-b36c-2924ed6a1670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cbcc5e-9471-402f-9b21-af768190042d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae67eb58-5181-42b9-8155-3469ef9a645b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c25ce4-41c9-4ac9-ba7c-862de6e084f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da7f9fb-07a3-4e62-8065-f7164b78946f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a932e5-84e2-47b9-a83a-813e333e823b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d4049-9a65-4b6c-acc7-c5e2c063643f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2150b2-951f-41c4-b2f4-75c5b502efbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb5020-1f3f-474d-859b-3c0f756946dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe104d3-ad2d-4684-b4b4-6bf216b10db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d28e37-f28a-451e-bfff-f08e6b0219e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31aa7c1-7a6a-4b15-a170-e5e8f345a417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d2a27f-f26d-4189-a96d-a7dda79d1943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b77a0d-1aa6-411d-999f-3233a8df36c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40aeb9d-dcd5-45ae-ae74-cbb2820eadaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9663770a-e48b-4350-babd-77df828203f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94923dec-aab1-442f-b1ec-532d13ab9ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf174ff1-6d8a-42e9-889a-b477e1cb1d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf9ca3a-eefb-4497-9d44-0ce83383e064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d8bdb-af7b-4df7-a1b2-ee366a9229b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc43b3cc-67f5-4ff5-aa9d-0fe576f2e31e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12760bf-2370-4a8e-a549-db85a732758d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77096eee-0454-4ffc-b84b-cfb6827d576e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ca2346-eb78-4d49-a56f-76d3dec6ee6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b958a24-f396-492b-ba7d-33831e28f682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e69f76-487a-4c17-9edd-7721e25d6dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab63b080-734e-4068-90c3-0034718bf4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6463e3-62f8-4baf-afe8-0466775aad86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d702b985-8f89-4786-a086-3caad3e18ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316e6342-f46d-42f4-b30f-5a7fc18e5bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d30b1db-b870-4bd5-a08c-6cea256e5518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbb18e4-3d83-4c10-a686-420c3ef701ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e5de08-24f5-4257-b9cf-348c812ca9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
